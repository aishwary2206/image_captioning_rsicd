{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1733066-4545-4cc0-94c3-e4a6791dbb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.3.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting pillow\n",
      "  Downloading pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.23.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.13/site-packages (4.67.1)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/miniconda3/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.13/site-packages (from torch) (78.1.1)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading numpy-2.3.3-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.8.0-cp313-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.23.0-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, pillow, numpy, networkx, fsspec, filelock, torch, torchvision\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9\u001b[0m [torchvision]\u001b[0m [torchvision]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.19.1 fsspec-2025.9.0 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 pillow-11.3.0 sympy-1.14.0 torch-2.8.0 torchvision-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pillow torch torchvision tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69dac60c-e522-4f89-b75f-6e366300dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e43923f-ce80-49f7-8a38-0296692c17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_JSON = \"../dataset_rsicd.json\"   # go up one folder to find dataset file\n",
    "IMAGE_DIR = \"../RSICD_images/\"        # go up one folder to find images\n",
    "OUTPUT_DIR = \"../preprocessed/\"       # save preprocessed folder alongside dataset\n",
    "\n",
    "MIN_WORD_FREQ = 5\n",
    "MAX_SEQ_LEN = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67275963-15ea-4808-ad4d-56add8901bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.path.exists(DATA_JSON))   # should print True\n",
    "print(os.path.exists(IMAGE_DIR))   # should print True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c066328c-9f71-45a2-8c0d-f9634c52b5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10921 image entries\n",
      "Example entry keys: dict_keys(['filename', 'imgid', 'sentences', 'split', 'sentids'])\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_JSON, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(dataset['images'])} image entries\")\n",
    "print(\"Example entry keys:\", dataset['images'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50257c28-e439-4bc6-81dc-db88a4d6b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned caption: many planes are parked next to a long building in an airport\n",
      "Total captions: 54605\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "\n",
    "def clean_caption(caption):\n",
    "    caption = caption.lower()\n",
    "    caption = re.sub(f\"[{string.punctuation}]\", \"\", caption)\n",
    "    return caption.strip()\n",
    "\n",
    "all_captions = []\n",
    "for img_entry in dataset['images']:\n",
    "    for sent in img_entry['sentences']:\n",
    "        raw = \" \".join(sent['tokens'])\n",
    "        all_captions.append(clean_caption(raw))\n",
    "\n",
    "print(\"Sample cleaned caption:\", all_captions[0])\n",
    "print(\"Total captions:\", len(all_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a1eb59e-5d27-4155-900a-fc94544473a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 1304\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "for cap in all_captions:\n",
    "    counter.update(cap.split())\n",
    "\n",
    "# keep only frequent words\n",
    "words = [w for w, c in counter.items() if c >= MIN_WORD_FREQ]\n",
    "\n",
    "# add special tokens\n",
    "word_map = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n",
    "for i, w in enumerate(words, start=4):\n",
    "    word_map[w] = i\n",
    "\n",
    "rev_word_map = {v: k for k, v in word_map.items()}\n",
    "\n",
    "print(\"Vocab size:\", len(word_map))\n",
    "\n",
    "# make sure folder exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# save vocab\n",
    "with open(os.path.join(OUTPUT_DIR, \"word_to_idx.json\"), \"w\") as f:\n",
    "    json.dump(word_map, f)\n",
    "with open(os.path.join(OUTPUT_DIR, \"idx_to_word.json\"), \"w\") as f:\n",
    "    json.dump(rev_word_map, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a196e92-c650-49a0-a0e5-af9072c28e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenized captions.\n"
     ]
    }
   ],
   "source": [
    "captions_dict = {}\n",
    "\n",
    "for img_entry in dataset['images']:\n",
    "    img_id = img_entry['filename']\n",
    "    captions_dict[img_id] = []\n",
    "    for sent in img_entry['sentences']:\n",
    "        tokens = [word_map.get(w, word_map[\"<unk>\"]) \n",
    "                  for w in clean_caption(\" \".join(sent['tokens'])).split()]\n",
    "        \n",
    "        # wrap with <start>, <end>\n",
    "        tokens = [word_map[\"<start>\"]] + tokens[:MAX_SEQ_LEN-2] + [word_map[\"<end>\"]]\n",
    "        \n",
    "        # pad sequence\n",
    "        while len(tokens) < MAX_SEQ_LEN:\n",
    "            tokens.append(word_map[\"<pad>\"])\n",
    "        \n",
    "        captions_dict[img_id].append(tokens)\n",
    "\n",
    "# save\n",
    "with open(os.path.join(OUTPUT_DIR, \"captions_tokens.json\"), \"w\") as f:\n",
    "    json.dump(captions_dict, f)\n",
    "\n",
    "print(\"Saved tokenized captions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dce5f28-1899-4999-996d-a03607bbc1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /Users/aishwaryshree/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 97.8M/97.8M [00:11<00:00, 8.63MB/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "modules = list(resnet.children())[:-1]  # remove last FC layer\n",
    "resnet = nn.Sequential(*modules)\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9868c04a-46c1-4911-ae4a-9737ade1f7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 10921/10921 [05:22<00:00, 33.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image features and mapping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_map = {}\n",
    "features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, img_entry in tqdm(enumerate(dataset['images']), total=len(dataset['images'])):\n",
    "        img_id = img_entry['filename']\n",
    "        img_path = os.path.join(IMAGE_DIR, img_id)\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        \n",
    "        image_map[img_id] = idx\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "        \n",
    "        feat = resnet(img_tensor).squeeze().cpu().numpy()\n",
    "        features.append(feat)\n",
    "\n",
    "features = np.array(features)\n",
    "\n",
    "# save\n",
    "np.save(os.path.join(OUTPUT_DIR, \"image_features.npy\"), features)\n",
    "with open(os.path.join(OUTPUT_DIR, \"image_map.json\"), \"w\") as f:\n",
    "    json.dump(image_map, f)\n",
    "\n",
    "print(\"Saved image features and mapping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04c06de9-f24b-4ca8-a480-47849f932570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset splits.\n"
     ]
    }
   ],
   "source": [
    "splits = {\"train\": [], \"val\": [], \"test\": []}\n",
    "\n",
    "for img_entry in dataset['images']:\n",
    "    img_id = img_entry['filename']\n",
    "    split = img_entry['split']\n",
    "    splits[split].append(img_id)\n",
    "\n",
    "for split in splits:\n",
    "    with open(os.path.join(OUTPUT_DIR, f\"{split}.json\"), \"w\") as f:\n",
    "        json.dump(splits[split], f)\n",
    "\n",
    "print(\"Saved dataset splits.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
